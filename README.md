# Ask GPT using Nano Dataset

This project implements a GPT (Generative Pre-trained Transformer) model using a nano dataset for demonstration purposes. The code is designed to showcase the functionality of GPT models, including tokenization, embedding, attention mechanisms and text generation.

The goal is to determine the minimum amount of training data required to achieve effective results with a minimal dataset.

Spoiler: the model occasionally produces answers, but results can vary.

## Nano Dataset

The nano dataset consists of simple sentences about capitals and countries, designed for quick experimentation and testing.

## Presentation

The accompanying PDF provides some explanations of how GPT and transformer models work.